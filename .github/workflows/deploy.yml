# ── Rating Platform — Build, Push & Deploy (ECS Fargate) ─────────────────────
#
# Triggers:
#   - Manual dispatch only → choose environment (dev / staging / prod)
#
# GitHub Secrets required (Settings > Secrets > Actions):
#   AWS_ACCESS_KEY_ID       - IAM credentials with ECR + ECS + RDS + Terraform perms
#   AWS_SECRET_ACCESS_KEY
#   DB_PASSWORD             - RDS password (Terraform creates RDS with this password)
#
# All non-sensitive config (vpc_id, subnets, ecs_cluster_name, etc.) lives in
# infra/terraform/variables.tf with defaults. Edit that file to change infra config.

name: Deploy

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment"
        required: true
        default: "dev"
        type: choice
        options: [dev, staging, prod]
      skip_terraform:
        description: "Skip Terraform apply (images only)"
        required: false
        default: false
        type: boolean
      skip_build_push:
        description: "Skip Build & Push (use existing images; run only Migrations and/or Deploy)"
        required: false
        default: false
        type: boolean
      image_tag_override:
        description: "Image tag when skipping build (e.g. b9929023 from a previous run)"
        required: false
        default: "latest"
        type: string
      run_migrations:
        description: "Run DB migrations after Terraform apply (in deploy job)"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1
  TF_DIR: infra/terraform
  ECS_CLUSTER: sre-poc-mcp-cluster

jobs:
  # ────────────────────────────────────────────────────────────────────────────
  # 1. Build & push all Docker images to ECR
  # ────────────────────────────────────────────────────────────────────────────
  build-push:
    name: Build & Push Images
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.tag.outputs.tag || steps.tag-skip.outputs.tag }}
    steps:
      - name: Set image tag (when skipping build)
        id: tag-skip
        if: ${{ inputs.skip_build_push }}
        run: echo "tag=${{ inputs.image_tag_override || 'latest' }}" >> "$GITHUB_OUTPUT"

      - uses: actions/checkout@v4
        if: ${{ !inputs.skip_build_push }}

      - name: Set image tag
        id: tag
        if: ${{ !inputs.skip_build_push }}
        run: echo "tag=${GITHUB_SHA::8}" >> "$GITHUB_OUTPUT"

      - uses: actions/setup-node@v4
        if: ${{ !inputs.skip_build_push }}
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies
        if: ${{ !inputs.skip_build_push }}
        run: npm ci

      - name: Configure AWS credentials
        if: ${{ !inputs.skip_build_push }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        id: ecr-login
        if: ${{ !inputs.skip_build_push }}
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repositories if missing
        if: ${{ !inputs.skip_build_push }}
        run: |
          REPOS=(
            rating-platform/core-rating
            rating-platform/line-rating
            rating-platform/product-config
            rating-platform/transform-service
            rating-platform/rules-service
            rating-platform/status-service
            rating-platform/rating-workspace
            rating-platform/adapter-kafka
            rating-platform/adapter-dnb
            rating-platform/adapter-gw
          )
          for repo in "${REPOS[@]}"; do
            aws ecr describe-repositories --repository-names "$repo" --region "${{ env.AWS_REGION }}" 2>/dev/null ||
            aws ecr create-repository --repository-name "$repo" --region "${{ env.AWS_REGION }}" --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE
          done

      - name: Build and push all images
        if: ${{ !inputs.skip_build_push }}
        env:
          ECR_REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          IMAGE_TAG: ${{ steps.tag.outputs.tag }}
        run: |
          SERVICES=(
            "orchestrators/core-rating/Dockerfile:core-rating:rating-platform/core-rating"
            "orchestrators/line-rating/Dockerfile:line-rating:rating-platform/line-rating"
            "services/product-config/Dockerfile:product-config:rating-platform/product-config"
            "services/transform-service/Dockerfile:transform-service:rating-platform/transform-service"
            "services/rules-service/Dockerfile:rules-service:rating-platform/rules-service"
            "services/status-service/Dockerfile:status-service:rating-platform/status-service"
            "frontend/rating-workspace/Dockerfile:rating-workspace:rating-platform/rating-workspace"
            "services/adapters/kafka/Dockerfile:adapter-kafka:rating-platform/adapter-kafka"
            "services/adapters/dnb/Dockerfile:adapter-dnb:rating-platform/adapter-dnb"
            "services/adapters/gw/Dockerfile:adapter-gw:rating-platform/adapter-gw"
          )

          for entry in "${SERVICES[@]}"; do
            IFS=: read -r dockerfile nx_project ecr_name <<< "$entry"
            image_uri="${ECR_REGISTRY}/${ecr_name}:${IMAGE_TAG}"

            echo "::group::Build $nx_project"
            if [ "$nx_project" = "rating-workspace" ]; then
              npx nx run rating-workspace:build
            else
              npx nx run "$nx_project":build
              npx nx run "$nx_project":prune-lockfile 2>/dev/null || true
              npx nx run "$nx_project":copy-workspace-modules 2>/dev/null || true
            fi
            docker build -f "$dockerfile" -t "$image_uri" .
            docker push "$image_uri"
            echo "::endgroup::"
            echo "✓ $nx_project → $image_uri"
          done

  # ────────────────────────────────────────────────────────────────────────────
  # 2. Terraform plan + apply (deploys ECS); migrations run after apply when requested
  # ────────────────────────────────────────────────────────────────────────────
  deploy:
    name: Terraform Deploy
    needs: [build-push]
    if: |
      always() &&
      needs.build-push.result == 'success' &&
      inputs.skip_terraform != true
    runs-on: ubuntu-latest
    timeout-minutes: 25
    environment: ${{ inputs.environment || 'dev' }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7"

      - name: Terraform init
        working-directory: ${{ env.TF_DIR }}
        env:
          TF_STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
        run: |
          if [ -n "${TF_STATE_BUCKET}" ]; then
            echo "Using remote state in s3://${TF_STATE_BUCKET}"
            terraform init -input=false \
              -backend-config="bucket=${TF_STATE_BUCKET}" \
              -backend-config="key=rating-platform/${{ inputs.environment || 'dev' }}/terraform.tfstate" \
              -backend-config="region=${{ env.AWS_REGION }}" \
              -backend-config="dynamodb_table=rating-platform-tfstate-locks" \
              -backend-config="encrypt=true"
          else
            echo "::warning::TF_STATE_BUCKET not set — using local state. State will not persist between runs; add the variable for remote state. See infra/terraform/README.md#remote-state."
            terraform init -input=false -backend=false
          fi

      - name: Terraform plan
        working-directory: ${{ env.TF_DIR }}
        env:
          TF_VAR_image_tag: ${{ needs.build-push.outputs.image_tag }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        run: terraform plan -input=false -out=tfplan

      - name: Terraform apply
        working-directory: ${{ env.TF_DIR }}
        env:
          TF_VAR_image_tag: ${{ needs.build-push.outputs.image_tag }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        run: terraform apply -input=false -auto-approve tfplan

      - name: Run DB migrations
        if: ${{ inputs.run_migrations }}
        timeout-minutes: 3
        env:
          DB_USER: rating_user
          DB_PASS: ${{ secrets.DB_PASSWORD }}
          DB_NAME: rating_platform
        run: |
          sudo apt-get update -qq && sudo apt-get install -y -qq postgresql-client
          DB_HOST=$(cd ${{ env.TF_DIR }} && terraform output -raw rds_endpoint)
          DB_PORT=$(cd ${{ env.TF_DIR }} && terraform output -raw rds_port)
          export DB_HOST DB_PORT PGPASSWORD="$DB_PASS"
          # Fail fast if RDS unreachable (e.g. private subnet); timeout prevents runner from hanging
          timeout 120 ./scripts/run-migrations.sh || { echo "::error::Migrations failed or timed out. If RDS is in a private subnet, run migrations from inside the VPC (e.g. bastion or one-off ECS task)."; exit 1; }

      - name: Force new deployment for all services
        run: |
          SERVICES=(
            core-rating line-rating product-config
            transform-service rules-service status-service
            rating-workspace adapter-kafka adapter-dnb adapter-gw
          )
          for svc in "${SERVICES[@]}"; do
            echo "Forcing deployment: $svc"
            aws ecs update-service \
              --cluster "${{ env.ECS_CLUSTER }}" \
              --service "$svc" \
              --force-new-deployment \
              --region "${{ env.AWS_REGION }}" \
              --no-cli-pager || true
          done

      - name: Show deploy info
        working-directory: ${{ env.TF_DIR }}
        run: |
          echo "───────────────────────────────────────────"
          echo "Deploy complete!"
          echo "Image tag:    ${{ needs.build-push.outputs.image_tag }}"
          echo "ECS Cluster:  $(terraform output -raw ecs_cluster_name 2>/dev/null || echo 'N/A')"
          echo "ALB DNS:      $(terraform output -raw alb_dns_name 2>/dev/null || echo 'N/A')"
          echo "───────────────────────────────────────────"

  # ────────────────────────────────────────────────────────────────────────────
  # 4. Smoke test — verify ECS services are healthy after deploy
  # ────────────────────────────────────────────────────────────────────────────
  smoke-test:
    name: Smoke Test
    needs: [build-push, deploy]
    if: needs.deploy.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Wait for ECS services to stabilize
        run: |
          SERVICES=(
            core-rating line-rating product-config
            transform-service rules-service status-service
            rating-workspace adapter-kafka adapter-dnb adapter-gw
          )
          echo "Waiting for services to reach RUNNING state..."
          for svc in "${SERVICES[@]}"; do
            echo -n "  $svc: "
            aws ecs wait services-stable \
              --cluster "${{ env.ECS_CLUSTER }}" \
              --services "$svc" \
              --region "${{ env.AWS_REGION }}" 2>/dev/null \
              && echo "STABLE" || echo "TIMEOUT (may still be deploying)"
          done

      - name: ECS service status
        run: |
          echo "── ECS Services ─────────────────────────────"
          aws ecs describe-services \
            --cluster "${{ env.ECS_CLUSTER }}" \
            --services \
              core-rating line-rating product-config \
              transform-service rules-service status-service \
              rating-workspace adapter-kafka adapter-dnb adapter-gw \
            --region "${{ env.AWS_REGION }}" \
            --query 'services[].{Name:serviceName,Status:status,Running:runningCount,Desired:desiredCount,TaskDef:taskDefinition}' \
            --output table --no-cli-pager
